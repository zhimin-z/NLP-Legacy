{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文自然语言处理分析\n",
    "by Ethan\n",
    "\n",
    "和拉丁语系不同，亚洲语言是不用空格分开每个有意义的词的。而当我们进行自然语言处理的时候，大部分情况下，词汇是我们对句子和文章理解的基础，因此需要一个工具去把完整的文本中分解成粒度更细的词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于 TF-IDF 算法的关键词抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jieba.analyse\n",
    "\n",
    "* jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n",
    "    * sentence 为待提取的文本\n",
    "    * topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    "    * withWeight 为是否一并返回关键词权重值，默认值为 False\n",
    "    * allowPOS 仅包括指定词性的词，默认值为空，即不筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.251 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户  2016  互联网  手机  平台  人工智能  百度  2017  智能  技术  数据  360  服务  直播  产品  企业  安全  视频  移动  应用  网络  行业  游戏  机器人  电商  内容  中国  领域  通过  发展\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "content = \"\".join(lines)\n",
    "print \"  \".join(analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "航母  训练  海军  中国  官兵  部队  编队  10  作战  任务  美国  导弹  能力  20  2016  军事  无人机  装备  进行  记者  我们  军队  安全  保障  12  战略  军人  日本  南海  战机\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "content = \"\".join(lines)\n",
    "print \"  \".join(analyse.extract_tags(content, topK=30, withWeight=False, allowPOS=()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于 TextRank 算法的关键词抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。\n",
    "* jieba.analyse.TextRank() 新建自定义 TextRank 实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法论文： [TextRank: Bringing Order into Texts](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "\n",
    "基本思想:\n",
    "\n",
    "* 将待抽取关键词的文本进行分词\n",
    "* 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图\n",
    "* 计算图中节点的PageRank，注意是无向带权图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国  海军  训练  美国  部队  进行  官兵  航母  作战  任务  能力  军事  发展  工作  国家  问题  建设  导弹  编队  记者\n",
      "---------------------我是分割线----------------\n",
      "中国  海军  美国  部队  官兵  航母  军事  国家  任务  能力  导弹  技术  问题  日本  军队  编队  装备  系统  记者  战略\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/military_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "content = \"\".join(lines)\n",
    "\n",
    "print \"  \".join(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')))\n",
    "print \"---------------------我是分割线----------------\"\n",
    "print \"  \".join(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## LDA主题模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咱们来用LDA主题模型建模，看看这些新闻主要在说哪些topic。\n",
    "\n",
    "首先我们要把文本内容处理成固定的格式，一个包含句子的list，list中每个元素是分词后的词list。类似下面这个样子。\n",
    "\n",
    "[[第，一，条，新闻，在，这里],[第，二，条，新闻，在，这里],[这，是，在，做， 什么],...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "stopwords=stopwords['stopword'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换成合适的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/technology_news.csv\", encoding='utf-8')\n",
    "df = df.dropna()\n",
    "lines=df.content.values.tolist()\n",
    "\n",
    "sentences=[]\n",
    "for line in lines:\n",
    "    try:\n",
    "        segs=jieba.lcut(line)\n",
    "        segs = filter(lambda x:len(x)>1, segs)\n",
    "        segs = filter(lambda x:x not in stopwords, segs)\n",
    "        sentences.append(segs)\n",
    "    except Exception,e:\n",
    "        print line\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看一眼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本次\n",
      "商汤\n",
      "带来\n",
      "黄仁勋\n",
      "展示\n",
      "遥相呼应\n",
      "SenseFace\n",
      "人脸\n",
      "布控\n",
      "系统\n",
      "千万级\n",
      "人员\n",
      "库中\n",
      "300ms\n",
      "识别\n",
      "瞬间\n",
      "锁定目标\n",
      "功耗\n",
      "十几\n",
      "当属\n",
      "人脸\n",
      "布控\n",
      "一大\n",
      "科技\n"
     ]
    }
   ],
   "source": [
    "for word in sentences[5]:\n",
    "    print word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(sentences)\n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21, 1),\n",
       " (25, 1),\n",
       " (54, 1),\n",
       " (59, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 1),\n",
       " (91, 1),\n",
       " (103, 1),\n",
       " (104, 2),\n",
       " (105, 2),\n",
       " (112, 1),\n",
       " (126, 1),\n",
       " (130, 1),\n",
       " (131, 1),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 1),\n",
       " (138, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们查一下第3号分类，其中最常出现的单词是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040*\"产品\" + 0.016*\"品牌\" + 0.016*\"消费者\" + 0.015*\"市场\" + 0.012*\"体验\"\n"
     ]
    }
   ],
   "source": [
    "print lda.print_topic(3, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把所有的主题打印出来看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024*\"发展\" + 0.020*\"企业\" + 0.017*\"技术\" + 0.015*\"产业\" + 0.014*\"中国\" + 0.014*\"创新\" + 0.013*\"行业\" + 0.013*\"领域\"\n",
      "0.020*\"直播\" + 0.011*\"活动\" + 0.009*\"国美\" + 0.008*\"母婴\" + 0.007*\"现场\" + 0.006*\"比特\" + 0.006*\"电视\" + 0.006*\"生活\"\n",
      "0.014*\"软件\" + 0.010*\"云端\" + 0.010*\"时间\" + 0.008*\"文件\" + 0.008*\"小时\" + 0.008*\"隔离\" + 0.006*\"北京\" + 0.006*\"实时\"\n",
      "0.040*\"产品\" + 0.016*\"品牌\" + 0.016*\"消费者\" + 0.015*\"市场\" + 0.012*\"体验\" + 0.008*\"用户\" + 0.008*\"消费\" + 0.007*\"学校\"\n",
      "0.033*\"用户\" + 0.019*\"勒索\" + 0.018*\"信息\" + 0.018*\"手机\" + 0.016*\"攻击\" + 0.016*\"网络\" + 0.012*\"系统\" + 0.011*\"诈骗\"\n",
      "0.034*\"智能\" + 0.019*\"数据\" + 0.018*\"技术\" + 0.014*\"互联网\" + 0.013*\"服务\" + 0.011*\"企业\" + 0.011*\"提供\" + 0.010*\"平台\"\n",
      "0.023*\"增长\" + 0.021*\"公司\" + 0.017*\"亿元\" + 0.015*\"业务\" + 0.015*\"孩子\" + 0.015*\"收入\" + 0.013*\"家长\" + 0.012*\"同比\"\n",
      "0.018*\"流量\" + 0.011*\"微信\" + 0.010*\"高通\" + 0.010*\"知识产权\" + 0.007*\"蓝色\" + 0.007*\"量子\" + 0.006*\"费用\" + 0.006*\"4G\"\n",
      "0.061*\"百度\" + 0.043*\"人工智能\" + 0.025*\"技术\" + 0.022*\"VR\" + 0.012*\"学习\" + 0.010*\"永恒\" + 0.008*\"机器\" + 0.007*\"识别\"\n",
      "0.024*\"数据\" + 0.015*\"宽带\" + 0.007*\"防御\" + 0.007*\"测试\" + 0.007*\"物流\" + 0.007*\"提速\" + 0.006*\"公司\" + 0.006*\"资金\"\n",
      "0.012*\"摄像头\" + 0.010*\"美团\" + 0.010*\"外卖\" + 0.008*\"点评\" + 0.008*\"妈妈\" + 0.007*\"星际\" + 0.007*\"拍照\" + 0.006*\"课堂\"\n",
      "0.018*\"用户\" + 0.018*\"数据\" + 0.013*\"联想\" + 0.011*\"杨元庆\" + 0.011*\"老师\" + 0.010*\"医疗\" + 0.009*\"医生\" + 0.008*\"搜狗\"\n",
      "0.043*\"360\" + 0.024*\"漏洞\" + 0.020*\"手机\" + 0.009*\"网站\" + 0.008*\"QQ\" + 0.008*\"应急\" + 0.006*\"搜索\" + 0.006*\"修复\"\n",
      "0.019*\"电商\" + 0.013*\"共享\" + 0.010*\"平台\" + 0.009*\"城市\" + 0.008*\"政务\" + 0.007*\"数据\" + 0.007*\"数据中心\" + 0.007*\"报告\"\n",
      "0.010*\"信息安全\" + 0.008*\"地图\" + 0.007*\"人类\" + 0.006*\"人工智能\" + 0.006*\"员工\" + 0.006*\"实验室\" + 0.005*\"人才\" + 0.005*\"力量\"\n",
      "0.087*\"游戏\" + 0.012*\"玩家\" + 0.010*\"独立\" + 0.007*\"联盟\" + 0.006*\"学生\" + 0.006*\"杭州\" + 0.005*\"协议\" + 0.005*\"平台\"\n",
      "0.022*\"中国\" + 0.016*\"数据\" + 0.014*\"市场\" + 0.013*\"城市\" + 0.008*\"发展\" + 0.008*\"旅游\" + 0.008*\"建设\" + 0.008*\"战略\"\n",
      "0.026*\"中国\" + 0.020*\"腾讯\" + 0.017*\"创业\" + 0.011*\"2017\" + 0.009*\"全球\" + 0.008*\"日电\" + 0.008*\"电竞\" + 0.008*\"中新网\"\n",
      "0.052*\"手机\" + 0.013*\"市场\" + 0.012*\"苹果\" + 0.011*\"小米\" + 0.010*\"智能手机\" + 0.009*\"联想\" + 0.008*\"金立\" + 0.008*\"无人机\"\n",
      "0.034*\"内容\" + 0.028*\"平台\" + 0.027*\"视频\" + 0.024*\"用户\" + 0.019*\"病毒\" + 0.016*\"直播\" + 0.012*\"营销\" + 0.009*\"广告\"\n"
     ]
    }
   ],
   "source": [
    "for topic in lda.print_topics(num_topics=20, num_words=8):\n",
    "    print topic[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以对新加入的文本，进行简单主题分类：\n",
    "\n",
    "`lda.get_document_topics(bow)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
